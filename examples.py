"""
Script d'exemple avancÃ© montrant toutes les fonctionnalitÃ©s
avec optimisations de performance.
"""

from src.pipeline import DarijaDatasetPipeline
from src.logger import setup_logger
from src.optimization import performance_monitor, cache_manager
import time

logger = setup_logger(__name__)


def example_basic_usage():
    """Exemple basique: traiter des commentaires directement."""
    print("\n" + "="*60)
    print("EXEMPLE 1: Traitement Basique de Commentaires")
    print("="*60 + "\n")
    
    pipeline = DarijaDatasetPipeline(use_scraper=False)
    
    # Commentaires d'exemple
    comments = [
        {
            "text": "ÙˆØ§Ø­ Ø§Ù„Ø¨Ø¯Ø± ÙŠØ§ Ø³ÙŠØ¯ÙŠ! Ø´Ø­Ø§Ù„ Ø¯ÙŠØ§Ù„ Ø§Ù„Ø¬Ù…Ø§Ù„Ø© ÙÙŠ Ù‡Ø°Ø§ Ø§Ù„ÙˆÙ‚Øª",
            "url": "https://www.facebook.com/post/123"
        },
        {
            "text": "Ù…Ù„ÙŠØ­ Ø§Ù„Ø¨Ø²Ø§Ù ÙŠØ§ Ù„Ø®ÙˆÙŠ! ØºØ§Ø¯ÙŠ Ù†Ø´ÙˆÙÙƒ Ù‚Ø±ÙŠØ¨ Ø¥Ù† Ø´Ø§Ø¡ Ø§Ù„Ù„Ù‡",
            "url": "https://www.facebook.com/post/456"
        },
        {
            "text": "Ù†Ø­Ù† Ù†Ø¯Ø¹Ù… Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø§Ù„ÙØµØ­Ù‰ ÙˆØ§Ù„Ø¯Ø§Ø±Ø¬Ø© Ø§Ù„Ù…ØºØ±Ø¨ÙŠØ©",
            "url": "https://www.facebook.com/post/789"
        },
        {
            "text": "This is an English comment that will be filtered",
            "url": "https://www.facebook.com/post/123"
        },
        {
            "text": "Ø§Ù„Ø³Ù„Ø§Ù… Ø¹Ù„ÙŠÙƒÙ… ÙˆØ±Ø­Ù…Ø© Ø§Ù„Ù„Ù‡ ÙˆØ¨Ø±ÙƒØ§ØªÙ‡ØŒ ÙƒÙŠÙØ§Ø´ ØªØ§Ø¹ Ø§Ù„Ø­ÙˆØ§Ù„",
            "url": "https://www.facebook.com/post/1011"
        },
    ]
    
    logger.info(f"Traitement de {len(comments)} commentaires...")
    darija_count = pipeline.process_comments_batch(comments)
    
    pipeline.save_dataset()
    pipeline.print_statistics()
    
    return pipeline


def example_with_batching():
    """Exemple avec traitement par lots optimisÃ©."""
    print("\n" + "="*60)
    print("EXEMPLE 2: Traitement OptimisÃ© par Lots")
    print("="*60 + "\n")
    
    from src.optimization import OptimizedBatchProcessor
    from src.cleaner import DataCleaner
    
    cleaner = DataCleaner()
    
    # GÃ©nÃ©rer des commentaires de test
    sample_texts = [
        "ÙˆØ§Ø­ Ø§Ù„Ø¨Ø¯Ø± ÙŠØ§ Ø³ÙŠØ¯ÙŠ! Ø´Ø­Ø§Ù„ Ø¯ÙŠØ§Ù„ Ø§Ù„Ø¬Ù…Ø§Ù„Ø©",
        "Ù…Ù„ÙŠØ­ Ø§Ù„Ø¨Ø²Ø§Ù ÙŠØ§ Ù„Ø®ÙˆÙŠ! ØºØ§Ø¯ÙŠ Ù†Ø´ÙˆÙÙƒ Ù‚Ø±ÙŠØ¨",
        "This is English text",
        "Ù†Ø­Ù† Ù†Ø¯Ø¹Ù… Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø§Ù„ÙØµØ­Ù‰",
        "Bonjour mon ami, Ã§a va?",
        "Ø§Ù„Ø³Ù„Ø§Ù… Ø¹Ù„ÙŠÙƒÙ… ÙˆØ±Ø­Ù…Ø© Ø§Ù„Ù„Ù‡ ÙˆØ¨Ø±ÙƒØ§ØªÙ‡",
    ] * 100  # RÃ©pÃ©ter pour avoir 600 textes
    
    print(f"Nettoyage de {len(sample_texts)} textes par lots...\n")
    
    start_time = time.time()
    cleaned_texts = OptimizedBatchProcessor.process_with_batching(
        cleaner.clean_batch,
        sample_texts,
        batch_size=100
    )
    elapsed = time.time() - start_time
    
    print(f"âœ“ {len(cleaned_texts)} textes nettoyÃ©s en {elapsed:.2f}s")
    print(f"âœ“ Performance: {len(sample_texts)/elapsed:.0f} textes/seconde\n")


def example_language_detection():
    """Exemple de dÃ©tection de langue avec cache."""
    print("\n" + "="*60)
    print("EXEMPLE 3: DÃ©tection de Langue avec Cache")
    print("="*60 + "\n")
    
    from src.language_detector import LanguageDetector
    
    detector = LanguageDetector()
    
    test_texts = [
        "ÙˆØ§Ø­ Ø§Ù„Ø¨Ø¯Ø± ÙŠØ§ Ø³ÙŠØ¯ÙŠ! Ø´Ø­Ø§Ù„ Ø¯ÙŠØ§Ù„ Ø§Ù„Ø¬Ù…Ø§Ù„Ø©",
        "Ù…Ù„ÙŠØ­ Ø§Ù„Ø¨Ø²Ø§Ù ÙŠØ§ Ù„Ø®ÙˆÙŠ",
        "This is an English text",
        "Bonjour mon ami",
        # RÃ©pÃ©ter pour dÃ©montrer le cache
        "ÙˆØ§Ø­ Ø§Ù„Ø¨Ø¯Ø± ÙŠØ§ Ø³ÙŠØ¯ÙŠ! Ø´Ø­Ø§Ù„ Ø¯ÙŠØ§Ù„ Ø§Ù„Ø¬Ù…Ø§Ù„Ø©",
        "Ù…Ù„ÙŠØ­ Ø§Ù„Ø¨Ø²Ø§Ù ÙŠØ§ Ù„Ø®ÙˆÙŠ",
    ]
    
    print("DÃ©tection de langue:")
    for text in test_texts:
        lang, confidence = detector.detect_language(text)
        is_darija = detector.is_darija(text)
        print(f"  {text[:40]:<42} â†’ {lang} (conf: {confidence:.2f}, Darija: {is_darija})")
    
    print("\n" + "Cache Statistics:")
    cache_stats = cache_manager.get_stats()
    print(f"  Size: {cache_stats['size']}")
    print(f"  Hits: {cache_stats['hits']}")
    print(f"  Misses: {cache_stats['misses']}")
    print(f"  Hit Rate: {cache_stats['hit_rate']:.2f}%\n")


def example_csv_operations():
    """Exemple d'opÃ©rations CSV."""
    print("\n" + "="*60)
    print("EXEMPLE 4: OpÃ©rations CSV")
    print("="*60 + "\n")
    
    from src.csv_manager import CSVManager
    
    csv_mgr = CSVManager("data/example_output.csv")
    
    # Ajouter des enregistrements
    records = [
        {"text": "ÙˆØ§Ø­ Ø§Ù„Ø¨Ø¯Ø± ÙŠØ§ Ø³ÙŠØ¯ÙŠ!", "url": "https://facebook.com/1"},
        {"text": "Ù…Ù„ÙŠØ­ Ø§Ù„Ø¨Ø²Ø§Ù", "url": "https://facebook.com/2"},
        {"text": "Ù†Ø­Ù† Ù†Ø¯Ø¹Ù… Ø§Ù„Ø¯Ø§Ø±Ø¬Ø©", "url": "https://facebook.com/3"},
        {"text": "ÙƒÙŠÙØ§Ø´ ØªØ§Ø¹ Ø§Ù„Ø­ÙˆØ§Ù„", "url": "https://facebook.com/4"},
    ]
    
    csv_mgr.add_records(records)
    
    print(f"AjoutÃ© {len(records)} enregistrements\n")
    print("Sauvegarde du CSV...")
    output_file = csv_mgr.save_to_csv()
    
    print(f"âœ“ Fichier sauvegardÃ©: {output_file}\n")
    
    # Statistiques
    stats = csv_mgr.get_statistics()
    print("Statistiques du CSV:")
    print(f"  Total records: {stats['total_records']}")
    print(f"  Unique URLs: {stats['unique_urls']}")
    print(f"  Avg text length: {stats['avg_text_length']:.2f}\n")


def example_full_pipeline():
    """Exemple du pipeline complet optimisÃ©."""
    print("\n" + "="*60)
    print("EXEMPLE 5: Pipeline Complet OptimisÃ©")
    print("="*60 + "\n")
    
    start_time = time.time()
    
    # CrÃ©er le pipeline
    pipeline = DarijaDatasetPipeline(use_scraper=False)
    
    # GÃ©nÃ©rer des commentaires d'exemple
    comments = []
    sample_texts_darija = [
        "ÙˆØ§Ø­ Ø§Ù„Ø¨Ø¯Ø± ÙŠØ§ Ø³ÙŠØ¯ÙŠ! Ø´Ø­Ø§Ù„ Ø¯ÙŠØ§Ù„ Ø§Ù„Ø¬Ù…Ø§Ù„Ø©",
        "Ù…Ù„ÙŠØ­ Ø§Ù„Ø¨Ø²Ø§Ù ÙŠØ§ Ù„Ø®ÙˆÙŠ! ØºØ§Ø¯ÙŠ Ù†Ø´ÙˆÙÙƒ Ù‚Ø±ÙŠØ¨",
        "Ù†Ø­Ù† Ù†Ø¯Ø¹Ù… Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¯Ø§Ø±Ø¬Ø© Ø§Ù„Ù…ØºØ±Ø¨ÙŠØ©",
        "Ø§Ù„Ø³Ù„Ø§Ù… Ø¹Ù„ÙŠÙƒÙ… ÙˆØ±Ø­Ù…Ø© Ø§Ù„Ù„Ù‡ ÙˆØ¨Ø±ÙƒØ§ØªÙ‡",
        "ÙƒÙŠÙØ§Ø´ ØªØ§Ø¹ Ø§Ù„Ø­ÙˆØ§Ù„ ÙŠØ§ ØµØ¯ÙŠÙ‚ÙŠ",
        "ÙˆØ§Ø´ ÙƒØ§ÙŠÙ† Ø´ÙŠ Ù…Ø´Ø§ÙƒÙ„ØŸ",
        "ÙˆÙ„Ø§Ù‡ ÙŠØ§ Ø³ÙŠØ¯ÙŠØŒ ÙƒÙ„Ø´ÙŠ Ù…Ù„ÙŠØ­",
        "ØºØ§Ø¯ÙŠ Ù†ØªÙ„Ø§Ù‚Ø§Ùˆ Ù‚Ø±ÙŠØ¨ Ø¥Ù† Ø´Ø§Ø¡ Ø§Ù„Ù„Ù‡",
        "Ø´Ù†Ùˆ Ø§Ù„Ø£Ø®Ø¨Ø§Ø± ÙŠØ§ ØµØ­Ø§Ø¨ÙŠ",
        "Ø­Ù†Ø§ ÙÙ‚Ø±Ø§Ø¡ ÙˆØ¹Ù†Ø¯Ù†Ø§ Ù„Ù‚Ù…Ø© Ø§Ù„Ø¹ÙŠØ´",
    ]
    
    for i, text in enumerate(sample_texts_darija * 5):  # 50 commentaires
        comments.append({
            "text": text + f" (comment {i+1})",
            "url": f"https://www.facebook.com/post/{i % 5 + 1}"
        })
    
    print(f"Traitement de {len(comments)} commentaires...")
    print(f"(Chaque texte sera nettoyÃ©, filtrÃ© et vÃ©rifiÃ©)")
    print()
    
    # Traiter
    darija_count = pipeline.process_comments_batch(comments)
    
    # Sauvegarder
    output_file = pipeline.save_dataset()
    
    # Statistiques
    elapsed = time.time() - start_time
    pipeline.print_statistics()
    
    print(f"\nâœ“ Temps total: {elapsed:.2f}s")
    print(f"âœ“ Vitesse de traitement: {len(comments)/elapsed:.0f} comments/sec\n")


def example_performance_analysis():
    """Analyse dÃ©taillÃ©e de la performance."""
    print("\n" + "="*60)
    print("EXEMPLE 6: Analyse de Performance")
    print("="*60 + "\n")
    
    from src.cleaner import DataCleaner
    from src.language_detector import LanguageDetector
    import time
    
    cleaner = DataCleaner()
    detector = LanguageDetector()
    
    # Texte de test
    test_text = "ÙˆØ§Ø­ Ø§Ù„Ø¨Ø¯Ø± ÙŠØ§ Ø³ÙŠØ¯ÙŠ! Ø´Ø­Ø§Ù„ Ø¯ÙŠØ§Ù„ Ø§Ù„Ø¬Ù…Ø§Ù„Ø© ÙÙŠ Ù‡Ø°Ø§ Ø§Ù„ÙˆÙ‚Øª ğŸ˜Š"
    
    # Benchmark nettoyage
    start = time.time()
    for _ in range(1000):
        cleaner.clean(test_text)
    clean_time = time.time() - start
    
    # Benchmark dÃ©tection
    cleaned = cleaner.clean(test_text)
    start = time.time()
    for _ in range(1000):
        detector.detect_language(cleaned)
    detect_time = time.time() - start
    
    print("Benchmark (1000 itÃ©rations):")
    print(f"  Nettoyage: {clean_time:.2f}s ({1000/clean_time:.0f} ops/sec)")
    print(f"  DÃ©tection: {detect_time:.2f}s ({1000/detect_time:.0f} ops/sec)")
    print()


def main():
    """ExÃ©cute tous les exemples."""
    print("\n" + "="*70)
    print(" "*15 + "DARIJA DATASET BUILDER - EXEMPLES AVANCÃ‰S")
    print("="*70)
    
    try:
        # Example 1: Basic usage
        example_basic_usage()
        
        # Example 2: Optimized batching
        example_with_batching()
        
        # Example 3: Language detection with cache
        example_language_detection()
        
        # Example 4: CSV operations
        example_csv_operations()
        
        # Example 5: Full pipeline
        example_full_pipeline()
        
        # Example 6: Performance analysis
        example_performance_analysis()
        
        print("\n" + "="*70)
        print("âœ“ Tous les exemples ont Ã©tÃ© exÃ©cutÃ©s avec succÃ¨s!")
        print("="*70 + "\n")
        
    except Exception as e:
        logger.error(f"Erreur lors de l'exÃ©cution des exemples: {e}")
        raise


if __name__ == "__main__":
    main()
