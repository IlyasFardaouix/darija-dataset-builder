# -*- coding: utf-8 -*-
"""
YouTube Comment Scraper ‚Äî Collecteur de commentaires Darija depuis YouTube.

Utilise l'API YouTube Data v3 pour extraire massivement les commentaires
des vid√©os marocaines populaires (musique, sport, actualit√©s, cuisine, etc.).

Quota API : 10,000 unit√©s/jour (gratuit).
- commentThreads.list = 1 unit√© par requ√™te (100 commentaires max)
- search.list = 100 unit√©s par requ√™te
- Estimation : ~900k commentaires/jour de quota th√©orique
"""

import os
import time
import json
import requests
from pathlib import Path
from typing import List, Dict, Generator, Optional, Set
from dotenv import load_dotenv
from src.logger import setup_logger

load_dotenv()
logger = setup_logger(__name__)

# ============================================================================
# CONFIGURATION YOUTUBE
# ============================================================================

YOUTUBE_API_BASE = "https://www.googleapis.com/youtube/v3"
API_KEY = os.getenv("YOUTUBE_API_KEY", "")

# Cha√Ænes YouTube marocaines populaires ‚Äî couvrent sport, musique, news, cuisine, humour
MOROCCAN_CHANNELS = [
    # === MUSIQUE & DIVERTISSEMENT ===
    "UC2PMGuYGaGMiGSHPv4RlJbQ",   # Saad Lamjarred
    "UCY1kMZp36IQSyNx_9h4mpCg",   # Fnaire
    "UCbNz1j6MhtJfGXsJlTVShig",   # Douzi
    "UC8aG3LDTDwNR1UQhSn9uVrw",   # Hatim Ammor
    "UCEjOF0H_y5v6h-fcJrCqImw",   # Zouhair Bahaoui
    "UCq-Fj5jknLsUf-MWSy4_brA",   # RotanaMusic (vid√©os marocaines)
    # === SPORT ===
    "UCp3JD_O13dU7rG8bSxvlAaQ",   # beIN Sports MENA
    "UCCEdmXagMjguJJh2VLK2xRA",   # Arryadia
    # === NEWS & ACTUS ===
    "UCk8ixMxOMXFCnnbAmgGS5Yw",   # Hespress
    "UCBb1BmP0gCp5wuDJJ424G6w",   # 2M Maroc
    "UCSAB-Y4kDz8cFxiJFcbPekA",   # Al Aoula
    "UCW1gG7t5-aSPb1xE-sU79eg",   # Medi1 TV
    "UCR6PjMGDPxSTQi4NfKJkiCA",   # Chouf TV
    "UC6Qg1RNjzYBFdRWVuRY6kbw",   # Le360
    # === HUMOUR & LIFESTYLE ===
    "UCphiYMYD3P7pQCFPaFx0lMA",   # Amine Raghib
    "UC3hxMVBYDZAIi4E30yKnc-Q",   # Mehdi Mozayine
    "UCoY-2raC0HqWgREfFPd3loA",   # Rachid Show
    # === CUISINE MAROCAINE ===
    "UCVAUiCeaFvpzWpvRlHVB-6w",   # Choumicha
    "UCYLclwCrhFQ51BOkA_l2DZg",   # Cuisine Marocaine
]

# Termes de recherche YouTube pour trouver des vid√©os avec commentaires Darija
SEARCH_QUERIES_DARIJA = [
    # Sport & Football
    "ÿßŸÑŸÖŸÜÿ™ÿÆÿ® ÿßŸÑŸÖÿ∫ÿ±ÿ®Ÿä",
    "ÿßŸÑŸàÿØÿßÿØ ÿßŸÑÿ±ÿ¨ÿßÿ° ÿßŸÑÿØŸäÿ±ÿ®Ÿä",
    "ÿ£ÿ≥ŸàÿØ ÿßŸÑÿ£ÿ∑ŸÑÿ≥ ŸÉÿ£ÿ≥ ÿßŸÑÿπÿßŸÑŸÖ",
    "ÿ≠ŸÉŸäŸÖŸä ÿ®ŸàŸÜŸà ÿ£ŸÖÿ±ÿßÿ®ÿ∑",
    "ÿßŸÑÿ®ÿ∑ŸàŸÑÿ© ÿßŸÑŸÖÿ∫ÿ±ÿ®Ÿäÿ©",
    "raja wydad match",
    # Musique
    "ÿ£ÿ∫ÿßŸÜŸä ŸÖÿ∫ÿ±ÿ®Ÿäÿ© ÿ¥ÿπÿ®Ÿäÿ©",
    "ÿ±ÿßŸä ŸÖÿ∫ÿ±ÿ®Ÿä ÿ¨ÿØŸäÿØ",
    "ŸÉŸÜÿßŸàÿ© ŸÖŸàÿ≥ŸäŸÇŸâ",
    "ÿ≥ÿπÿØ ŸÑŸÖÿ¨ÿ±ÿØ ÿ¨ÿØŸäÿØ",
    # Cuisine
    "ÿ∑ÿ®ÿÆ ŸÖÿ∫ÿ±ÿ®Ÿä ÿ™ŸÇŸÑŸäÿØŸä",
    "ÿ∑ÿßÿ¨ŸäŸÜ ŸÖÿ∫ÿ±ÿ®Ÿä ŸàÿµŸÅÿ©",
    "ŸÉÿ≥ŸÉÿ≥ ŸÖÿ∫ÿ±ÿ®Ÿä",
    "ÿ≠ÿ±Ÿäÿ±ÿ© ÿ±ŸÖÿ∂ÿßŸÜ",
    # Actualit√©s Maroc
    "ÿ£ÿÆÿ®ÿßÿ± ÿßŸÑŸÖÿ∫ÿ±ÿ® ÿßŸÑŸäŸàŸÖ",
    "Ÿáÿ≥ÿ®ÿ±Ÿäÿ≥ ÿ¨ÿØŸäÿØ",
    "ÿßŸÑÿ≠Ÿäÿßÿ© ŸÅŸä ÿßŸÑŸÖÿ∫ÿ±ÿ®",
    "ŸÖÿ∫ÿßÿ±ÿ®ÿ© ÿßŸÑÿπÿßŸÑŸÖ",
    # Humour & Lifestyle
    "ŸÉŸàŸÖŸäÿØŸäÿß ŸÖÿ∫ÿ±ÿ®Ÿäÿ© ŸÖÿ∂ÿ≠ŸÉÿ©",
    "ŸÅŸäÿØŸäŸàŸáÿßÿ™ ŸÖÿ∫ÿ±ÿ®Ÿäÿ© ŸÖÿ∂ÿ≠ŸÉÿ©",
    "ÿßŸÑÿ≠Ÿäÿßÿ© ŸÅÿßŸÑŸÖÿ∫ÿ±ÿ®",
    "darija maroc drole",
    # Culture
    "ÿ™ÿßÿ±ŸäÿÆ ÿßŸÑŸÖÿ∫ÿ±ÿ®",
    "ŸÖÿØŸÜ ÿßŸÑŸÖÿ∫ÿ±ÿ® ÿ≥Ÿäÿßÿ≠ÿ©",
    "ÿßŸÑÿπÿ±ÿ≥ ÿßŸÑŸÖÿ∫ÿ±ÿ®Ÿä ÿ™ŸÇÿßŸÑŸäÿØ",
    "ÿ±ŸÖÿ∂ÿßŸÜ ŸÅÿßŸÑŸÖÿ∫ÿ±ÿ®",
    # Mix FR/AR/Darija
    "maroc vlog darija",
    "marocain reaction",
    "maghreb actualite",
]

# Fichier de sauvegarde JSONL brut
RAW_DATA_DIR = Path(__file__).parent.parent / "data" / "raw"
RAW_DATA_DIR.mkdir(parents=True, exist_ok=True)
YOUTUBE_RAW_FILE = RAW_DATA_DIR / "youtube_comments.jsonl"
YOUTUBE_PROGRESS_FILE = RAW_DATA_DIR / "youtube_progress.json"


class YouTubeScraper:
    """
    Collecteur de commentaires YouTube marocains via l'API Data v3.
    
    Strat√©gie de collecte:
    1. Rechercher des vid√©os par mots-cl√©s Darija/Maroc
    2. Lister les vid√©os des cha√Ænes marocaines populaires
    3. Extraire TOUS les commentaires + r√©ponses de chaque vid√©o
    4. Sauvegarder en JSONL brut avec pagination
    5. Respecter le quota API (10,000 unit√©s/jour)
    """
    
    def __init__(self, api_key: str = None):
        self.api_key = api_key or API_KEY
        if not self.api_key:
            raise ValueError(
                "YouTube API Key manquante! "
                "Ajoutez YOUTUBE_API_KEY dans le fichier .env"
            )
        self.session = requests.Session()
        self.quota_used = 0
        self.total_comments = 0
        self.processed_videos: Set[str] = set()
        self._load_progress()
    
    def _load_progress(self):
        """Charge la progression pr√©c√©dente pour reprendre l√† o√π on s'est arr√™t√©."""
        if YOUTUBE_PROGRESS_FILE.exists():
            try:
                with open(YOUTUBE_PROGRESS_FILE, 'r', encoding='utf-8') as f:
                    progress = json.load(f)
                self.processed_videos = set(progress.get("processed_videos", []))
                self.total_comments = progress.get("total_comments", 0)
                logger.info(
                    f"Progression charg√©e: {len(self.processed_videos)} vid√©os, "
                    f"{self.total_comments} commentaires"
                )
            except Exception as e:
                logger.warning(f"Erreur chargement progression: {e}")
    
    def _save_progress(self):
        """Sauvegarde la progression actuelle."""
        progress = {
            "processed_videos": list(self.processed_videos),
            "total_comments": self.total_comments,
            "quota_used": self.quota_used,
        }
        with open(YOUTUBE_PROGRESS_FILE, 'w', encoding='utf-8') as f:
            json.dump(progress, f, ensure_ascii=False, indent=2)
    
    def _api_request(self, endpoint: str, params: dict, cost: int = 1) -> Optional[dict]:
        """
        Fait une requ√™te √† l'API YouTube avec gestion d'erreurs et quota.
        
        Args:
            endpoint: Endpoint API (ex: "commentThreads")
            params: Param√®tres de la requ√™te
            cost: Co√ªt en unit√©s de quota
            
        Returns:
            R√©ponse JSON ou None en cas d'erreur
        """
        params["key"] = self.api_key
        url = f"{YOUTUBE_API_BASE}/{endpoint}"
        
        try:
            resp = self.session.get(url, params=params, timeout=30)
            self.quota_used += cost
            
            if resp.status_code == 200:
                return resp.json()
            elif resp.status_code == 403:
                error_reason = resp.json().get("error", {}).get("errors", [{}])[0].get("reason", "")
                if error_reason == "quotaExceeded":
                    logger.warning("‚ö†Ô∏è  Quota YouTube d√©pass√©! Attendez demain ou utilisez une autre cl√©.")
                    return None
                elif error_reason == "commentsDisabled":
                    logger.debug("Commentaires d√©sactiv√©s sur cette vid√©o")
                    return None
                else:
                    logger.warning(f"Acc√®s refus√©: {error_reason}")
                    return None
            elif resp.status_code == 404:
                logger.debug("Ressource non trouv√©e")
                return None
            else:
                logger.warning(f"Erreur API {resp.status_code}: {resp.text[:200]}")
                return None
                
        except requests.exceptions.Timeout:
            logger.warning(f"Timeout sur {endpoint}")
            return None
        except requests.exceptions.RequestException as e:
            logger.error(f"Erreur r√©seau: {e}")
            return None
    
    def search_videos(self, query: str, max_results: int = 50) -> List[str]:
        """
        Recherche des vid√©os YouTube par mot-cl√©.
        
        Args:
            query: Terme de recherche
            max_results: Nombre max de vid√©os (max 50 par requ√™te)
            
        Returns:
            Liste d'IDs de vid√©os
        """
        video_ids = []
        page_token = None
        
        while len(video_ids) < max_results:
            params = {
                "part": "id",
                "q": query,
                "type": "video",
                "maxResults": min(50, max_results - len(video_ids)),
                "regionCode": "MA",
                "relevanceLanguage": "ar",
                "order": "relevance",
            }
            if page_token:
                params["pageToken"] = page_token
            
            data = self._api_request("search", params, cost=100)
            if not data:
                break
            
            for item in data.get("items", []):
                vid_id = item.get("id", {}).get("videoId")
                if vid_id and vid_id not in self.processed_videos:
                    video_ids.append(vid_id)
            
            page_token = data.get("nextPageToken")
            if not page_token:
                break
        
        logger.info(f"Recherche '{query}': {len(video_ids)} vid√©os trouv√©es")
        return video_ids
    
    def get_channel_videos(self, channel_id: str, max_results: int = 200) -> List[str]:
        """
        R√©cup√®re les vid√©os d'une cha√Æne YouTube.
        
        Args:
            channel_id: ID de la cha√Æne
            max_results: Nombre max de vid√©os
            
        Returns:
            Liste d'IDs de vid√©os
        """
        video_ids = []
        page_token = None
        
        while len(video_ids) < max_results:
            params = {
                "part": "id",
                "channelId": channel_id,
                "type": "video",
                "maxResults": min(50, max_results - len(video_ids)),
                "order": "viewCount",
            }
            if page_token:
                params["pageToken"] = page_token
            
            data = self._api_request("search", params, cost=100)
            if not data:
                break
            
            for item in data.get("items", []):
                vid_id = item.get("id", {}).get("videoId")
                if vid_id and vid_id not in self.processed_videos:
                    video_ids.append(vid_id)
            
            page_token = data.get("nextPageToken")
            if not page_token:
                break
        
        logger.info(f"Cha√Æne {channel_id}: {len(video_ids)} vid√©os")
        return video_ids
    
    def get_video_comments(self, video_id: str, max_comments: int = 0) -> Generator[Dict, None, None]:
        """
        Extrait TOUS les commentaires + r√©ponses d'une vid√©o.
        
        Args:
            video_id: ID de la vid√©o YouTube
            max_comments: Limite (0 = illimit√©)
            
        Yields:
            Dict {"text": ..., "url": ..., "source": "youtube", ...}
        """
        page_token = None
        comment_count = 0
        video_url = f"https://www.youtube.com/watch?v={video_id}"
        
        while True:
            params = {
                "part": "snippet,replies",
                "videoId": video_id,
                "maxResults": 100,
                "textFormat": "plainText",
                "order": "relevance",
            }
            if page_token:
                params["pageToken"] = page_token
            
            data = self._api_request("commentThreads", params, cost=1)
            if not data:
                break
            
            for item in data.get("items", []):
                # Commentaire principal
                snippet = item.get("snippet", {}).get("topLevelComment", {}).get("snippet", {})
                text = snippet.get("textDisplay", "").strip()
                if text:
                    yield {
                        "text": text,
                        "url": video_url,
                        "source": "youtube",
                        "author": snippet.get("authorDisplayName", ""),
                        "likes": snippet.get("likeCount", 0),
                        "published": snippet.get("publishedAt", ""),
                    }
                    comment_count += 1
                
                # R√©ponses au commentaire
                replies = item.get("replies", {}).get("comments", [])
                for reply in replies:
                    reply_snippet = reply.get("snippet", {})
                    reply_text = reply_snippet.get("textDisplay", "").strip()
                    if reply_text:
                        yield {
                            "text": reply_text,
                            "url": video_url,
                            "source": "youtube",
                            "author": reply_snippet.get("authorDisplayName", ""),
                            "likes": reply_snippet.get("likeCount", 0),
                            "published": reply_snippet.get("publishedAt", ""),
                        }
                        comment_count += 1
                
                if max_comments and comment_count >= max_comments:
                    return
            
            page_token = data.get("nextPageToken")
            if not page_token:
                break
        
        self.processed_videos.add(video_id)
        self.total_comments += comment_count
        logger.debug(f"Vid√©o {video_id}: {comment_count} commentaires")
    
    def _save_comments_jsonl(self, comments: Generator[Dict, None, None]) -> int:
        """Sauvegarde les commentaires en JSONL (append)."""
        count = 0
        with open(YOUTUBE_RAW_FILE, 'a', encoding='utf-8') as f:
            for comment in comments:
                f.write(json.dumps(comment, ensure_ascii=False) + '\n')
                count += 1
        return count
    
    def scrape_all(self, max_videos_per_query: int = 30,
                   max_videos_per_channel: int = 100,
                   max_comments_per_video: int = 0,
                   save_interval: int = 5) -> int:
        """
        Lance la collecte compl√®te des commentaires YouTube marocains.
        
        Strat√©gie:
        1. Vid√©os des cha√Ænes marocaines populaires
        2. Recherche par mots-cl√©s Darija
        
        Args:
            max_videos_per_query: Vid√©os par requ√™te de recherche
            max_videos_per_channel: Vid√©os par cha√Æne
            max_comments_per_video: Limite par vid√©o (0=illimit√©)
            save_interval: Sauvegarder la progression toutes les N vid√©os
            
        Returns:
            Nombre total de commentaires collect√©s
        """
        all_video_ids = []
        
        print(f"\n{'='*60}")
        print(f"  üé¨ YOUTUBE SCRAPER ‚Äî Collecte de commentaires Darija")
        print(f"{'='*60}\n")
        
        # √âtape 1: Vid√©os des cha√Ænes marocaines
        print(f"üì∫ √âtape 1: Extraction des vid√©os de {len(MOROCCAN_CHANNELS)} cha√Ænes marocaines...")
        for channel_id in MOROCCAN_CHANNELS:
            try:
                vids = self.get_channel_videos(channel_id, max_videos_per_channel)
                all_video_ids.extend(vids)
                time.sleep(0.2)
            except Exception as e:
                logger.warning(f"Erreur cha√Æne {channel_id}: {e}")
        
        print(f"   ‚Üí {len(all_video_ids)} vid√©os depuis les cha√Ænes\n")
        
        # √âtape 2: Recherche par mots-cl√©s
        print(f"üîç √âtape 2: Recherche par {len(SEARCH_QUERIES_DARIJA)} mots-cl√©s Darija...")
        for query in SEARCH_QUERIES_DARIJA:
            try:
                vids = self.search_videos(query, max_videos_per_query)
                all_video_ids.extend(vids)
                time.sleep(0.2)
            except Exception as e:
                logger.warning(f"Erreur recherche '{query}': {e}")
        
        # D√©dupliquer les IDs
        all_video_ids = list(dict.fromkeys(all_video_ids))
        # Retirer les d√©j√† trait√©s
        all_video_ids = [v for v in all_video_ids if v not in self.processed_videos]
        
        print(f"   ‚Üí {len(all_video_ids)} vid√©os uniques √† traiter\n")
        
        if not all_video_ids:
            print("‚úÖ Toutes les vid√©os ont d√©j√† √©t√© trait√©es!")
            return self.total_comments
        
        # √âtape 3: Extraction des commentaires
        print(f"üí¨ √âtape 3: Extraction des commentaires de {len(all_video_ids)} vid√©os...\n")
        
        session_comments = 0
        for i, video_id in enumerate(all_video_ids, 1):
            try:
                comments = self.get_video_comments(video_id, max_comments_per_video)
                count = self._save_comments_jsonl(comments)
                session_comments += count
                
                if i % save_interval == 0:
                    self._save_progress()
                    print(f"   [{i}/{len(all_video_ids)}] "
                          f"Session: {session_comments:,} | "
                          f"Total: {self.total_comments:,} | "
                          f"Quota: ~{self.quota_used:,} unit√©s")
                
                time.sleep(0.1)  # Respecter le rate limit
                
            except Exception as e:
                logger.warning(f"Erreur vid√©o {video_id}: {e}")
                continue
        
        self._save_progress()
        
        print(f"\n{'='*60}")
        print(f"  ‚úÖ YouTube Scraping termin√©!")
        print(f"  üìä Commentaires cette session: {session_comments:,}")
        print(f"  üìä Total cumul√©: {self.total_comments:,}")
        print(f"  üìÅ Fichier brut: {YOUTUBE_RAW_FILE}")
        print(f"  üí∞ Quota utilis√©: ~{self.quota_used:,} unit√©s / 10,000")
        print(f"{'='*60}\n")
        
        return session_comments
    
    def read_raw_comments(self) -> Generator[Dict, None, None]:
        """Lit les commentaires bruts depuis le fichier JSONL."""
        if not YOUTUBE_RAW_FILE.exists():
            return
        with open(YOUTUBE_RAW_FILE, 'r', encoding='utf-8') as f:
            for line in f:
                line = line.strip()
                if line:
                    try:
                        yield json.loads(line)
                    except json.JSONDecodeError:
                        continue
    
    def get_raw_count(self) -> int:
        """Compte le nombre de commentaires bruts collect√©s."""
        if not YOUTUBE_RAW_FILE.exists():
            return 0
        count = 0
        with open(YOUTUBE_RAW_FILE, 'r', encoding='utf-8') as f:
            for _ in f:
                count += 1
        return count
